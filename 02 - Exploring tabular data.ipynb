{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b388bb08-e46e-49b0-bfde-823060140c0e",
   "metadata": {},
   "source": [
    "# Exploring Tabular Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42074db-8e80-436f-9d76-952a84f74365",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Characteristics of Rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4a7256-646f-484e-9ced-67a6f4051e8f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Independent and Identically Distributed (IID)\n",
    "\n",
    "When working with tabular data, it's important to ensure that each row (i.e., example) is **independent and identically distributed (IID)** — unless you're dealing with time series or hierarchical data. IID means that each sample is drawn independently from the same underlying distribution, like repeated coin flips or dice rolls. This assumption simplifies many aspects of machine learning, such as model training, cross-validation, and sampling methods like bootstrapping. \n",
    "\n",
    "However, many real-world datasets violate the IID assumption. For instance, sales from the same store or responses from students in the same class tend to be correlated, introducing non-IID characteristics. Non-IID data can lead to misleading model performance, overfitting to hidden relationships, and biased validation scores. It's important to detect these patterns through data exploration and understand how they might impact your modeling process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79a6149-d2b8-4056-ad20-9bb105849d8b",
   "metadata": {},
   "source": [
    "#### Why IID Matters in Machine Learning\n",
    "\n",
    "Most statistical and machine learning methods assume that data is **independent and identically distributed (IID)**. While machine learning is often data-driven and nonparametric, it still relies heavily on the IID assumption to make reliable predictions. In reality, most datasets are not truly IID.\n",
    "\n",
    "A key limitation of machine learning algorithms is that they are **column-aware but not row-aware**—they learn relationships between features and the target but cannot understand dependencies between rows. This means that if data has hidden patterns due to time or grouping, the model may wrongly interpret them as feature-based correlations.\n",
    "\n",
    "**Time series and longitudinal data** are classic examples of non-IID data, where observations are autocorrelated. In such cases, time-based features (e.g., timestamps or lagged variables) help the model account for temporal dependencies. Depending on your data structure, you can:\n",
    "\n",
    "* Use timestamps as features (for proper time series analysis with special validation techniques).\n",
    "* Create time-based features by pivoting multiple time points into separate columns, allowing the data to be treated as IID.\n",
    "\n",
    "Properly identifying and handling non-IID structures is essential to avoid misleading results and to ensure your models generalize well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70941013-f404-4ef2-aa88-3e0f0c0196c2",
   "metadata": {},
   "source": [
    "#### Handling Non-IID Data in Time and Groups\n",
    "\n",
    "Even in **cross-sectional datasets**, comparing different time periods can introduce **temporal dependencies**, making the data non-IID—even when there's no interaction between units or groups. In such cases, the **order of observations matters**, and time series models become necessary to capture the correlation across time points.\n",
    "\n",
    "To prepare data effectively for modeling:\n",
    "\n",
    "* **Check how time affects your data**, and consider using **time features**, **lags**, or **moving averages** to control for temporal shifts.\n",
    "* Be **explicit about grouping**: if hidden relationships or groups exist in your data, represent them with features and use **group-aware validation techniques**.\n",
    "* For grouped data, prefer **group cross-validation** to avoid splitting related rows across train and validation sets.\n",
    "* For temporal data, use **time-based validation** methods to preserve the order and avoid data leakage.\n",
    "\n",
    "Properly addressing these issues improves model reliability and prevents overfitting to spurious or time-based patterns.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c9a65-291b-4ca4-8bb2-b9a8c31aa176",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. Characteristics of Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa7721d-d2a5-4cb2-b112-be1f942d7e78",
   "metadata": {},
   "source": [
    "#### A. Types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f0784d-6c69-4b1a-83b0-0c77c080acbd",
   "metadata": {},
   "source": [
    "It is important to understand the types of data that can be dealth with. Each type requires a special type of processing and needs specific way of dealing with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf1f946-98d8-4468-a06c-c34ee50dd0a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "| **Feature Type**      | **Description**                                                             | **Examples**                     | **Key Notes**                                                           |\n",
    "| --------------------- | --------------------------------------------------------------------------- | -------------------------------- | ----------------------------------------------------------------------- |\n",
    "| **Numeric (Float)**   | Real numbers; can be **ratio** (true zero) or **interval** (arbitrary zero) | Price, weight, temperature       | Can standardize; ensure consistent units; interval data can be negative |\n",
    "| **Numeric (Integer)** | Whole numbers; may also be **ordinal** or **categorical**                   | Count, age                       | Check continuity & uniqueness; treat carefully if used as labels        |\n",
    "| **Ordinal**           | Ordered categories with **uneven spacing**                                  | Ratings (e.g., 1–5 stars), ranks | Do not compute mean/std; preserve order but not magnitude               |\n",
    "| **Categorical**       | Unordered labels (strings or integers)                                      | Color, country, product type     | Handle with encoding; manage cardinality (low vs. high)                 |\n",
    "| **Binary**            | Special case of categorical with **two values**                             | Yes/No, 0/1, presence/absence    | Often used directly in models                                           |\n",
    "| **Date/Time**         | Temporal data                                                               | Timestamps, dates                | Decompose into parts (day, month, year); can convert to Unix time       |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94beebaa-c470-4886-a0c3-1fd9e262e2e1",
   "metadata": {},
   "source": [
    "#### B. Example of pandas dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f9447-f43b-40e0-82b2-870ba342b4d8",
   "metadata": {},
   "source": [
    "Pandas DataFrames are the standard structure in Python for managing tabular data with mixed types (numeric, categorical, dates, etc.). They offer flexible access by row or column labels, support data cleaning, transformation, and visualization, and are now fully supported throughout Scikit-learn pipelines, keeping outputs as DataFrames instead of converting to NumPy arrays. The book uses pandas extensively to explore and fix common tabular data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2fadb1c-afcf-4d02-b2f7-713ceba2913b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Listing 2.1 Creating a simple tabular dataset\n",
    "import pandas as pd\n",
    "\n",
    "data = {'gender': ['male', 'female', 'male', 'female'],\n",
    "        'age': [25, 30, 27, 29],\n",
    "        'education_level': ['Bachelor', 'Master', 'Bachelor', 'PhD'],\n",
    "        'income': [50000, 60000, 55000, 70000]} \n",
    "\n",
    "index = [ 'Bob ', 'Alice', 'Charlie', 'Emily'] \n",
    "\n",
    "df = pd.DataFrame(data, index=index) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccbcda59-0148-4ee2-ae6c-adc7df890906",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>education_level</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>male</td>\n",
       "      <td>25</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>female</td>\n",
       "      <td>30</td>\n",
       "      <td>Master</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlie</th>\n",
       "      <td>male</td>\n",
       "      <td>27</td>\n",
       "      <td>Bachelor</td>\n",
       "      <td>55000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>female</td>\n",
       "      <td>29</td>\n",
       "      <td>PhD</td>\n",
       "      <td>70000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         gender  age education_level  income\n",
       "Bob        male   25        Bachelor   50000\n",
       "Alice    female   30          Master   60000\n",
       "Charlie    male   27        Bachelor   55000\n",
       "Emily    female   29             PhD   70000"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2c1dff1-1c39-4933-b6d7-cd5060a727f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print specific row and column based on index and column name\n",
    "df.loc['Alice','age']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ba095783-fcdd-409e-adfc-e2ed1f39f36a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender             female\n",
       "age                    30\n",
       "education_level    Master\n",
       "income              60000\n",
       "Name: Alice, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print row with index \"Alex\"\n",
    "df.loc['Alice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa38d79-2631-4f0f-ae77-6c521b27077f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gender                 male\n",
       "age                      25\n",
       "education_level    Bachelor\n",
       "income                50000\n",
       "Name: Bob , dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print 1st row\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe86e44-a2c3-43d5-b94d-43bf0e3c90ad",
   "metadata": {},
   "source": [
    "#### C. Example of sklearn's pandas support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35d33ad9-1880-4d3e-81d2-9e00716f6e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bob</th>\n",
       "      <td>-1.432078</td>\n",
       "      <td>-1.183216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Alice</th>\n",
       "      <td>1.171700</td>\n",
       "      <td>0.169031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Charlie</th>\n",
       "      <td>-0.390567</td>\n",
       "      <td>-0.507093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Emily</th>\n",
       "      <td>0.650945</td>\n",
       "      <td>1.521278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              age    income\n",
       "Bob     -1.432078 -1.183216\n",
       "Alice    1.171700  0.169031\n",
       "Charlie -0.390567 -0.507093\n",
       "Emily    0.650945  1.521278"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "x = df[['age','income']]\n",
    "scaler.fit(x)\n",
    "X_test_scaled = scaler.transform(x)\n",
    "X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10df0aee-31c2-4d94-9a7f-7ba6e64a704b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Pathologies and Remedies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f679ca-2982-4d00-9d8a-6ba973660f8f",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df7d37-91c5-4c9a-bd14-3ec6e8be3a35",
   "metadata": {},
   "source": [
    "In tabular datasets, certain feature-related issues must always be avoided, as they can severely impact model performance. Historical examples like the Madelon dataset and recent Kaggle competitions demonstrate how noise, redundancy, irrelevant or collinear features, and flawed labels can make prediction extremely difficult—even for advanced models. These challenges highlight the importance of identifying and addressing common data issues, which will be explored in more detail using artificial and curated examples later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3938353-8d2b-4c49-adad-5fdfc746a8c1",
   "metadata": {},
   "source": [
    "### a. Constant or quasi-constant columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64e8dff-bc3d-4f93-a814-16043522fadb",
   "metadata": {},
   "source": [
    "Constant or quasi-constant columns should be removed because they provide no useful variability for learning and can lead to inefficiencies or overfitting in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aff034a8-2339-4c8c-b375-3e997b446cc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>B</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  feature_1 feature_2  feature_3\n",
       "0         A         B          0\n",
       "1         A         C          1\n",
       "2         A         B          4\n",
       "3         A         C          9\n",
       "4         A         B         16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###################################################\n",
    "### Listing 2.2 Dropping zero variance features ###\n",
    "###################################################\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create data \n",
    "data = pd.DataFrame({\"feature_1\":['A' for i in range(5)],\n",
    "                     \"feature_2\":['B' if i%2==0 else 'C' for i in range(5)],\n",
    "                     \"feature_3\":[i**2 for i in range(5)]})\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11f883fd-07e1-40ac-af2f-9b5aac98d47e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   feature_1  feature_2  feature_3\n",
       "0        0.0        0.0        0.0\n",
       "1        0.0        1.0        1.0\n",
       "2        0.0        0.0        2.0\n",
       "3        0.0        1.0        3.0\n",
       "4        0.0        0.0        4.0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# an ordinal encoder will transform your data from string labels to ordered numeric ones\n",
    "ord_enc = OrdinalEncoder() #A\n",
    "data[data.columns] = ord_enc.fit_transform(data)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e2442fe-73ce-4f09-962a-b49696b4da9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0.],\n",
       "       [1., 1.],\n",
       "       [0., 2.],\n",
       "       [1., 3.],\n",
       "       [0., 4.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The VarianceThreshold class will filter all features whose variance is equal to or below the selected threshold\n",
    "var_threshold = VarianceThreshold(threshold=0)\n",
    "clean_data = var_threshold.fit_transform(data)\n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e231e60-90ca-4be8-b413-72e0498e294f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.24000000000000005, 2.0]\n",
      "(5, 2)\n"
     ]
    }
   ],
   "source": [
    "# you can have the variances of all features represented by the .variances_ attribute\n",
    "print(list(var_threshold.variances_))\n",
    "print(clean_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3376e047-42d2-40eb-b20c-46ac867823b1",
   "metadata": {},
   "source": [
    "### b. Duplicated and highly collinear features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66cd1ccf-7f6b-46b5-ae6b-e988edd64015",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "* **Remove duplicated features** immediately — they waste resources and add no value.\n",
    "* **Highly collinear features** can:\n",
    "\n",
    "  * Indicate a **causal relationship** — keep the *causative* one (identified via domain knowledge or causal analysis).\n",
    "  * Reflect a **latent factor** — if the true cause isn't in the data, keep the feature:\n",
    "\n",
    "    * Most related to the target,\n",
    "    * With better data quality (fewer errors, missing values, or outliers).\n",
    "\n",
    "Use domain knowledge and basic data analysis to decide which features to retain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cf5cf7-9f55-47da-a67a-e78addc61e67",
   "metadata": {},
   "source": [
    "#### Example of dealing with it \n",
    "\n",
    "We will use the `make_classification` command will create a sample dataset of slightly correlated features [see documentation](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_classification.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0e7715a-3515-4d82-aaea-4e54b01b2f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X = (100, 20)\n",
      "y = (100,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_classification\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "np.random.seed(0)\n",
    "\n",
    "# Creating a synthetic dataset\n",
    "X, _ = make_classification(n_redundant=0, n_repeated=0, random_state=0)\n",
    "print(f\"X = {X.shape}\\ny = {_.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0bfb1b-881e-4857-8522-aa630b555e1b",
   "metadata": {},
   "source": [
    "**The variance inflation factor (VIF)** is used to quantify how much a feature's information is shared with other features in a dataset, helping to detect multicollinearity by assessing each feature's redundancy in relation to all others—unlike Pearson correlation, which only measures pairwise relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3996044-f036-4243-bc59-090f3f3110f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VIF of feature 0 = 29.48\n",
      "VIF of feature 1 = 31.05\n",
      "VIF of feature 2 = 17.91\n",
      "VIF of feature 3 = 48.56\n",
      "VIF of feature 4 = 28.78\n",
      "VIF of feature 5 = 1.26\n",
      "VIF of feature 6 = 1.45\n",
      "VIF of feature 7 = 1.25\n",
      "VIF of feature 8 = 1.64\n",
      "VIF of feature 9 = 1.58\n",
      "VIF of feature 10 = 1.43\n",
      "VIF of feature 11 = 1.34\n",
      "VIF of feature 12 = 1.25\n",
      "VIF of feature 13 = 1.34\n",
      "VIF of feature 14 = 1.68\n",
      "VIF of feature 15 = 1.38\n",
      "VIF of feature 16 = 1.68\n",
      "VIF of feature 17 = 1.26\n",
      "VIF of feature 18 = 1.23\n",
      "VIF of feature 19 = 1.2\n",
      "VIF of feature 20 = 23.61\n",
      "VIF of feature 21 = 18.63\n",
      "VIF of feature 22 = 16.66\n",
      "VIF of feature 23 = 23.65\n",
      "VIF of feature 24 = 19.63\n",
      "VIF of feature 25 = 18.19\n",
      "VIF of feature 26 = 22.87\n",
      "VIF of feature 27 = 13.98\n",
      "VIF of feature 28 = 28.35\n",
      "VIF of feature 29 = 20.28\n"
     ]
    }
   ],
   "source": [
    "# create more correlated columns by resusing some columns and adding noise\n",
    "X = np.hstack([X, X[:,:5] + np.random.random((X.shape[0],5))])\n",
    "\n",
    "# Computing the variance inflation factor (VIF) to spot the features that has the least unique contribution\n",
    "vif = [variance_inflation_factor(X, i) for i in range(X.shape[1])]\n",
    "for i,j in enumerate(np.round(vif,2)):\n",
    "    print(f\"VIF of feature {i} = {j}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2f6c5a-b796-4275-9785-282baba54a4b",
   "metadata": {},
   "source": [
    "You can see how the set of feature which have high VIF values are the ones which might have multicollinearrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16b83be7-5660-41fc-967f-64e141318217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature 0 and 20 have r=0.966\n",
      "feature 1 and 21 have r=0.963\n",
      "feature 2 and 22 have r=0.947\n",
      "feature 3 and 23 have r=0.958\n",
      "feature 4 and 24 have r=0.964\n"
     ]
    }
   ],
   "source": [
    "# Checking for correlations\n",
    "for a in range(X.shape[1]): #iterate over columns\n",
    "    for b in range(X.shape[1]):\n",
    "        if a < b:\n",
    "            r = np.corrcoef(X[:, a], X[:, b])[0][1] # estimate correlation coeeficient it's in absolute value\n",
    "            if np.abs(r) > 0.90: #G \n",
    "                print(f\"feature {a} and {b} have r={r:0.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557d1cc3-ea29-4bac-85ac-8ecfbc24e301",
   "metadata": {},
   "source": [
    "### c. Irrelevant features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d0820-766c-47fb-a8bd-beeff94b58f3",
   "metadata": {},
   "source": [
    "Irrelevant features—those with little association to the target—should be eliminated early using domain knowledge or basic statistical tests, as they add noise, reduce model interpretability, and hurt performance; later, more advanced methods like feature randomization can confirm their lack of impact on predictive power and justify their removal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8124c5d5-e10d-43a7-ac00-0f1c55f530cd",
   "metadata": {},
   "source": [
    "### d. Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e624e092-84df-4f5d-b2e3-8cd9a8ceb887",
   "metadata": {},
   "source": [
    "Missing data can disrupt many machine learning models, though algorithms like XGBoost and LightGBM can handle it internally; otherwise, imputation—using data from the same or other columns—is needed. Importantly, missingness itself can carry valuable information, and tracking it with binary indicators can reveal meaningful patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5d0b0c-e05f-4ddf-86a7-6b2b74b88596",
   "metadata": {},
   "source": [
    "### e. Rare catageories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a215d0-8794-4e6f-b739-bc84b2d5d1d8",
   "metadata": {},
   "source": [
    "Rare categories in categorical features—those with few instances—can lead to overfitting and are best handled early by aggregating them into broader groups, often guided by domain knowledge; when dealing with high-cardinality features, target encoding or embeddings are more suitable, depending on the model type."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda5d02-5e5c-4ada-9921-ff983f1796eb",
   "metadata": {},
   "source": [
    "### f. Errors in data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea5dfe6-bcef-4241-a866-4a09783c174d",
   "metadata": {},
   "source": [
    "Detecting and correcting data errors like incongruencies or distortions requires deep understanding of the domain and data recording processes, as systematic errors—unlike random ones—can bias results and severely harm model performance, and no machine learning algorithm can fully compensate for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df50f0ec-ec7a-43d1-91e3-2939c9c36034",
   "metadata": {},
   "source": [
    "### g. Leakage features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4f3991-62d6-4b12-aca6-8357b0d5461b",
   "metadata": {},
   "source": [
    "Data leakage happens when your model uses information during training that it wouldn’t actually have at prediction time. This makes the model seem better than it really is. One common type is feature-level leakage, which often happens when features are created using data from the future—after the target event. To avoid this, make sure that all features are based on data available before or at the same time as the target. Check timestamps or metadata to confirm when each piece of data was created. This helps ensure your model can perform reliably in real-world scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47a0416-b92c-44c2-960d-5bdcface5601",
   "metadata": {},
   "source": [
    "## 4. Finding external and internal data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a114742-f77f-4155-b292-40ad053531d4",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a9e5be-af39-42a0-816a-0965e51e9e59",
   "metadata": {},
   "source": [
    "To find a dataset for a machine learning project, you must first clearly define your project's goal. Once you know what you're trying to predict (the target), you can search for data using tools like Google Dataset Search or Kaggle.\n",
    "\n",
    "Start by identifying the target variable, then look for predictor features that help predict it. You can begin with a few features and add more later, but without a target, you can’t start.\n",
    "\n",
    "Finding, understanding, and preparing the data is one of the most time-consuming steps in machine learning. It involves locating the data, understanding how it’s structured, and organizing it into a usable form for your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96297f9-add4-441d-a841-b9a31b20c523",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### a. Using Pandas and access data stores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1177b32-5e8b-439b-a733-4646e29fa4f8",
   "metadata": {},
   "source": [
    "Data for analysis is often scattered across various sources like Excel files or normalized data warehouses. To build a usable dataset, you typically need to assemble event, item, and dimension tables. While SQL is useful for combining relational data, **pandas** is more flexible for data science tasks, offering SQL-like operations with added capabilities like visualization, control over data transformations, and compatibility with parallel or distributed computing tools.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b21ad3-84a7-428f-b36d-d1d79f9ca587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_name</th>\n",
       "      <th>price</th>\n",
       "      <th>product_description</th>\n",
       "      <th>category</th>\n",
       "      <th>manufacturer</th>\n",
       "      <th>weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Product A</td>\n",
       "      <td>10.99</td>\n",
       "      <td>A great product</td>\n",
       "      <td>Category A</td>\n",
       "      <td>Manufacturer A</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Product B</td>\n",
       "      <td>20.99</td>\n",
       "      <td>A high-quality product</td>\n",
       "      <td>Category B</td>\n",
       "      <td>Manufacturer B</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Product C</td>\n",
       "      <td>15.99</td>\n",
       "      <td>A reliable product</td>\n",
       "      <td>Category C</td>\n",
       "      <td>Manufacturer C</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Product D</td>\n",
       "      <td>8.99</td>\n",
       "      <td>An affordable product</td>\n",
       "      <td>Category D</td>\n",
       "      <td>Manufacturer D</td>\n",
       "      <td>1.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   product_id product_name  price     product_description    category  \\\n",
       "0           1    Product A  10.99         A great product  Category A   \n",
       "1           2    Product B  20.99  A high-quality product  Category B   \n",
       "2           3    Product C  15.99      A reliable product  Category C   \n",
       "3           4    Product D   8.99   An affordable product  Category D   \n",
       "\n",
       "     manufacturer  weight  \n",
       "0  Manufacturer A     1.5  \n",
       "1  Manufacturer B     2.0  \n",
       "2  Manufacturer C     1.8  \n",
       "3  Manufacturer D     1.2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Listing 2.4 Merging datasets in pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "#A. the first table, containing prices\n",
    "df1 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n",
    "                    'product_name': ['Product A', 'Product B', 'Product C', 'Product D'],\n",
    "                    'price': [10.99, 20.99, 15.99, 8.99]}) \n",
    "\n",
    "#B the second table, containing descriptions\n",
    "df2 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n",
    "                 'product_description': ['A great product', 'A high-quality product', \n",
    "                                         'A reliable product', 'An affordable product'],\n",
    "                 'category': ['Category A', 'Category B', 'Category C', 'Category D']}) \n",
    "\n",
    "#C the third table, containing makers and characteristics\n",
    "df3 = pd.DataFrame({'product_id': [1, 2, 3, 4],\n",
    "                    'manufacturer': ['Manufacturer A', 'Manufacturer B', \n",
    "                                     'Manufacturer C', 'Manufacturer D'],\n",
    "                    'weight': [1.5, 2.0, 1.8, 1.2]}) \n",
    "\n",
    "#D merging the first two tables\n",
    "merged_df = pd.merge(df1, df2, on='product_id') \n",
    "\n",
    "#E merging the previous two joined tables with the third one\n",
    "merged_df = pd.merge(merged_df, df3, on='product_id') \n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2d84d-ee3e-4d44-bd82-b68590f4d010",
   "metadata": {},
   "source": [
    "\n",
    "Pandas is great for small to medium data but struggles with speed and memory on large datasets. Several tools extend pandas or improve performance for bigger data and parallel computing.\n",
    "\n",
    "**Pros & Cons of some tools**\n",
    "\n",
    "| Tool       | Pros                                                  | Cons                                        |\n",
    "|------------|--------------------------------------------------------|---------------------------------------------|\n",
    "| **pandas** | Easy to use, rich API                                 | Slow, not scalable, memory-limited          |\n",
    "| **Dask**   | Scales to big data, runs on clusters                  | Not 100% pandas-compatible                  |\n",
    "| **Ray**    | Fast parallel processing                               | Low-level, not a pandas replacement         |\n",
    "| **Modin**  | Drop-in pandas replacement, faster with Ray/Dask      | Incomplete API coverage                     |\n",
    "| **Vaex**   | Very fast, lazy loading, out-of-core                  | Limited functionality vs. pandas            |\n",
    "| **RAPIDS** | GPU-accelerated, fast on large arrays                 | Needs NVIDIA GPU, limited memory            |\n",
    "| **Spark**  | Best for big data, distributed computing              | Complex setup, slower for small jobs        |\n",
    "| **Polars** | Very fast, memory-efficient, supports lazy mode       | Different API, still evolving               |\n",
    "\n",
    "<br>\n",
    "\n",
    "Use **Modin** or **Dask** for large pandas workflows, **Vaex** or **Polars** for speed, **RAPIDS** for GPU use, and **Spark** for big data pipelines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec583dd-4a0f-4e63-93cf-84c28d9ddc92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "691915f6-24d9-4be6-b706-73be1755bb4a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### b. Internet data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da91c68-089e-4848-85b1-443f88129030",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Online Sources for Tabular Datasets\n",
    "\n",
    "\n",
    "\n",
    "| Website / Platform                  | URL                                                                                      | What It Provides                                                                                |\n",
    "| ----------------------------------- | ---------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| **UCI Machine Learning Repository** | [https://archive.ics.uci.edu](https://archive.ics.uci.edu)                               | Classic ML datasets used for research, education, and benchmarking                              |\n",
    "| **OpenML**                          | [https://www.openml.org](https://www.openml.org)                                         | Open, collaborative dataset and experiment sharing platform used by Scikit-learn                |\n",
    "| **Harvard Dataverse**               | [https://dataverse.harvard.edu](https://dataverse.harvard.edu)                           | Free academic/scientific datasets across many domains                                           |\n",
    "| **Reddit Datasets**                 | [https://www.reddit.com/r/datasets/](https://www.reddit.com/r/datasets/)                 | Community-driven data discovery forum for niche or unique datasets                              |\n",
    "| **Data Portals**                    | [http://dataportals.org/](http://dataportals.org/)                                       | Global open data portal index from governments and organizations                                |\n",
    "| **Open Data Monitor**               | [https://opendatamonitor.eu/](https://opendatamonitor.eu/)                               | European-centric open data portal aggregator                                                    |\n",
    "| **National Statistical Services**   | [https://mng.bz/eynQ](https://mng.bz/eynQ)                                               | Global directory of official statistical agencies via U.S. Census Bureau                        |\n",
    "| **Google Dataset Search**           | [https://datasetsearch.research.google.com/](https://datasetsearch.research.google.com/) | Search engine for datasets across the web, filterable by format, license, and recency           |\n",
    "| **Kaggle Datasets**                 | [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)                       | Large, active collection of datasets for ML; often includes documentation, code, and discussion |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eef900b8-45a6-4017-bfa6-73303dc02d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listing 2.5 Download of Auto MPG Data Set from UCI repository\n",
    "\n",
    "from io import StringIO #StringIO reads and writes an in-memory string buffer                \n",
    "import requests #requests is an HTTP library that can help you recover data from the Web                          \n",
    "import pandas as pd\n",
    "\n",
    "# UCI ML repo \n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/\"\n",
    "data = \"auto-mpg.data-original\"\n",
    "\n",
    "# derived from the documentation on the UCI machine learning repository\n",
    "columns = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \n",
    "           \"acceleration\", \"model_year\", \"origin\", \"car_name\"] \n",
    "\n",
    "# fixed-width data requires providing each feature with its start and end position in the input \n",
    "# 1 row = \"18.0  8.0  307.0  ...  chevrolet chevelle malibu\" -> specify the location for each field\n",
    "colspecs = [(0, 4), (6, 9), (12, 17), (23, 28), (34, 39), \n",
    "            (45, 49), (52, 55), (57, 59), (61, -2)] \n",
    "\n",
    "# the dataset is read from the Web by requests.get() and then turned into a string buffer \n",
    "data_ingestion = StringIO(requests.get(url + data).text) \n",
    "\n",
    "# reads a table of fixed-width formatted lines into a DataFrame\n",
    "data = pd.read_fwf(data_ingestion, colspecs=colspecs, names=columns)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "45bce158-b81e-4a59-b058-7555e56abea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin</th>\n",
       "      <th>car_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>23.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>2855.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.</td>\n",
       "      <td>oldsmobile starfire sx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>36.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>79.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>1825.0</td>\n",
       "      <td>18.6</td>\n",
       "      <td>77.0</td>\n",
       "      <td>2.</td>\n",
       "      <td>renault 5 gtl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>17.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>163.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>3140.0</td>\n",
       "      <td>13.6</td>\n",
       "      <td>78.0</td>\n",
       "      <td>2.</td>\n",
       "      <td>volvo 264gl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>2506.0</td>\n",
       "      <td>14.5</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3.</td>\n",
       "      <td>toyouta corona mark ii (sw)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>15.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>3432.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>75.0</td>\n",
       "      <td>1.</td>\n",
       "      <td>mercury monarch</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "279  23.8        4.0         151.0        85.0  2855.0          17.6   \n",
       "225  36.0        4.0          79.0        58.0  1825.0          18.6   \n",
       "282  17.0        6.0         163.0       125.0  3140.0          13.6   \n",
       "89   23.0        4.0         120.0        97.0  2506.0          14.5   \n",
       "161  15.0        6.0         250.0        72.0  3432.0          21.0   \n",
       "\n",
       "     model_year origin                     car_name  \n",
       "279        78.0     1.       oldsmobile starfire sx  \n",
       "225        77.0     2.                renault 5 gtl  \n",
       "282        78.0     2.                  volvo 264gl  \n",
       "89         72.0     3.  toyouta corona mark ii (sw)  \n",
       "161        75.0     1.              mercury monarch  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d36c044-db0e-4215-a9ba-a3652a3934ca",
   "metadata": {},
   "source": [
    "### c. Synthetic data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e046e5a9-d97b-4c6f-9b5d-585aa5ba7eda",
   "metadata": {},
   "source": [
    "When real data is limited, **synthetic data** helps by expanding and improving datasets for machine learning.\n",
    "\n",
    "**Why Use Synthetic Data**\n",
    "\n",
    "* Adds more samples (solves data scarcity)\n",
    "* Balances class distributions\n",
    "* Generates rare edge cases\n",
    "* Preserves privacy (no real identities)\n",
    "\n",
    "**Main Techniques**\n",
    "\n",
    "* Generative Adversarial Networks (GANs): Learn to mimic real data from random noise using a generator and discriminator.\n",
    "* Variational Autoencoders (VAEs): Compress and reconstruct data via a latent space to generate realistic new samples.\n",
    "* \n",
    "\n",
    "**Example**\n",
    "\n",
    "* Kaggle’s Tabular Playground Series uses synthetic data for competitions:\n",
    "  [kaggle.com/competitions](https://www.kaggle.com/competitions)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f282bd1-59f8-4a81-81d2-f50772fde5f3",
   "metadata": {},
   "source": [
    "#### Extra: SDV (Synthetic Data Vault): \n",
    "\n",
    "Open-source package from MIT using **Gaussian copulas**, **GANs**, and **VAEs** to generate high-quality synthetic tabular data. GitHub: [github.com/sdv-dev/SDV](https://github.com/sdv-dev/SDV) -> Paper: https://mng.bz/ga4V \n",
    "\n",
    "If you interested to see how it works and validated go to this [notebook](./extra_Synthetic_Data.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58707d-8316-43ee-9e59-a24dc600da91",
   "metadata": {},
   "source": [
    "## 5. Exploratory data analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba919961-08fd-498d-a5f9-686304c623e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
